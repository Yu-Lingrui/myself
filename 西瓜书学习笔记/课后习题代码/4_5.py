# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14XTRRLgtoQIU2Kdn_XtGnLCZPfrZFgpy

# 导包
"""

import time
import numpy as np
import pandas as pd
from graphviz import Digraph
from sklearn.model_selection import train_test_split

"""# 加载数据"""

L = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'target']
heart = pd.read_csv('heart.csv')[L]

heart.head()

"""# 构造数据集"""

seed = 22
heart1 = heart.sample(n=75, replace=False, random_state=seed)
heart2 = heart.sample(n=75, replace=False, random_state=seed)
heart3 = heart.sample(n=75, replace=False, random_state=seed)
heart4 = heart.sample(n=75, replace=False, random_state=seed)

train1, val1 = train_test_split(heart1, test_size=0.2)
train2, val2 = train_test_split(heart2, test_size=0.2)
train3, val3 = train_test_split(heart3, test_size=0.2)
train4, val4 = train_test_split(heart4, test_size=0.2)

heart1.head()

heart2.head()

heart3.head()

heart4.head()

"""# 定义决策树

## 计算信息熵
"""

def get_Ent(D, l):
    # D为数据集，l为标签所在列的名称
    labels = list(set(D[l]))
    Ent = 0
    for label in labels:
        p = len(D[D[l] == label]) / float(len(D))
        if p == 0:
            Ent = 0
        else:
            Ent = Ent + (p)*np.log2(p)
    return -Ent

"""## 计算信息增益"""

def get_Gain(D, a, l):
    # D为数据集，a为属性值，l为标签所在列的名称
    labels = list(set(D[a]))
    tmp = 0
    for label in labels:
        D_ = D[D[a] == label]
        tmp = tmp + len(D_) / float(len(D)) * get_Ent(D_, l)
    Gain = get_Ent(D, l) - tmp
    return Gain

"""## 计算基尼值"""

def get_Gini(D, l):
    # D为数据集，l为标签所在列的名称
    labels = list(set(D[l]))
    Gini = 1
    for label in labels:
        p = len(D[D[l] == label]) / float(len(D))
        Gini = Gini - p * p
    return Gini

"""## 计算基尼指数"""

def get_Gini_index(D, a, l):
    # D为数据集，a为属性值，l为标签所在列的名称
    labels = list(set(D[a]))
    Gini = 0
    for label in labels:
        D_ = D[D[a] == label]
        tmp = len(D_) / float(len(D)) * get_Gini(D_, l)
        Gini = Gini + tmp
    return Gini

"""## 寻找最好的划分数据集的方式"""

def chooseBestSplit(D, A, l, model):
    if model == 'Gini':
        Ginis = []
        for a in A:
            Ginis.append(get_Gini_index(D, a, l))
        # bestFeature: 使得到最大增益划分的属性。
        bestFeature = A[(np.argmin(Ginis))]
    if model == 'Gain':
        Gains = []
        for a in A:
            Gains.append(get_Gain(D, a, l))
        # bestFeature: 使得到最大增益划分的属性。
        bestFeature = A[(np.argmax(Gains))]
    return bestFeature

"""## 投票"""

def majorityCnt(D, l):
    if len(D) == 0:
        return "1"     
    else:
        labels = list(set(D[l]))
        counts = []
        for label in labels:
            count = len(D[D[l] == label])
            counts.append(count)
        return labels[np.argmax(counts)]

"""## 生成未剪枝决策树"""

def TreeGenerate(D, A, model, l="target"):
    # 如果D为空，则返回默认值
    if len(D) == 0:
        return '1'
    
    # 如果剩余的类别全相同,则返回
    if len(list(set(D[l]))) == 1:
        return list(set(D[l]))[0]
    
    # 如果只剩下类标签，投票返回
    if len(A) == 0:
        return majorityCnt(D, l)

    # 得到增益最大划分的属性
    bestFeature = chooseBestSplit(D, A, l, model)
    
    # 创建属性标注
    txt = bestFeature + "=" + "?"

    # 创建节点
    myTree = {txt: {}}

    A.remove(bestFeature) # 生成子树的时候要将已遍历的属性删去。
    
    global train1
    values = list(set(train1[bestFeature]))  # 最好的特征的类别列表
    for value in values:    # 标称型的属性值有几种，就要几个子树。
        B = A[:]
        subD = D[D[bestFeature] == value]
        myTree[txt][value] = TreeGenerate(subD, B, model)
    return myTree

"""## 生成预剪枝决策树"""

def PreTreeGenerate(D, val, A, model, l="target"):
    # 如果D为空，则返回默认值
    if len(D) == 0:
        return '1'
    
    # 如果剩余的类别全相同,则返回
    if len(list(set(D[l]))) == 1:
        return list(set(D[l]))[0]
    
    # 如果只剩下类标签，投票返回
    if len(A) == 0:
        return majorityCnt(D, l)
    
    # 得到增益最大划分的属性、阈值
    bestFeature = chooseBestSplit(D, A, l, model)
    
    # 创建属性标注
    txt = bestFeature + "=" + "?"

    # 创建节点
    myTree = {txt: {}}

    A.remove(bestFeature) # 生成子树的时候要将已遍历的属性删去。数值型不要删除。
    
    # 计算剪枝后正确率
    label = majorityCnt(D, l)
    aacc = 0
    for index, row in val.iterrows():
        if label == row[l]:
            aacc = aacc + 1
    if len(val):
        aftacc = float(aacc) / len(val)
    else:
        aftacc = 0

    global train1
    values = list(set(train1[bestFeature]))  # 最好的特征的类别列表
    
    # 计算剪枝前正确率
    pacc = 0
    for value in values:
        subD = D[D[bestFeature] == value]
        subval = val[val[bestFeature] == value]
        label = majorityCnt(subD, l)
        for index, row in subval.iterrows():
            if label == row[l]:
                pacc = pacc + 1

    if len(val):
        preacc = float(pacc) / len(val)
    else:
        preacc = 0
    
    
    # 如果剪枝后正确率未提高，则不剪枝，否则剪枝
    if aftacc <= preacc:
        for value in values:    # 标称型的属性值有几种，就要几个子树。
            B = A[:]
            subD = D[D[bestFeature] == value]
            subval = val[val[bestFeature] == value]
            myTree[txt][value] = PreTreeGenerate(subD, subval, B, model)
        return myTree
    else:
        return majorityCnt(D, l)

"""## 生成后剪枝决策树

"""

def AfterTreeGenerate(D, val, A, model, l="target"):
    # 如果D为空，则返回默认值
    if len(D) == 0:
        return '1'
    
    # 如果剩余的类别全相同,则返回
    if len(list(set(D[l]))) == 1:
        return list(set(D[l]))[0]
    
    # 如果只剩下类标签，投票返回
    if len(A) == 0:
        return majorityCnt(D, l)

    # 得到增益最大划分的属性、阈值
    bestFeature = chooseBestSplit(D, A, l, model)
    
    # 创建属性标注
    txt = bestFeature + "=" + "?"

    # 创建节点
    myTree = {txt: {}}
    A.remove(bestFeature) # 生成子树的时候要将已遍历的属性删去。数值型不要删除。
    
    global train1
    values = list(set(train1[bestFeature]))  # 最好的特征的类别列表
    
    # 判断子节点是否为叶节点
    F = 1
    for value in values:
        B = A[:]
        subD = D[D[bestFeature] == value]
        subval = val[val[bestFeature] == value]
        if isinstance(AfterTreeGenerate(subD, subval, B, model), dict):
            F = 0

    if F==1:
        # 计算剪枝后正确率
        label = majorityCnt(D, l)
        aacc = 0
        for index, row in val.iterrows():
            if label == row[l]:
                aacc = aacc + 1
        if len(val):
            aftacc = float(aacc) / len(val)
        else:
            aftacc = 0
    
        # 计算剪枝前正确率
        pacc = 0
        for value in values:
            subD = D[D[bestFeature] == value]
            subval = val[val[bestFeature] == value]
            label = majorityCnt(subD, l)
            for index, row in subval.iterrows():
                if label == row[l]:
                    pacc = pacc + 1
        if len(val):
            preacc = float(pacc) / len(val)
        else:
            preacc = 0
        
        # 如果剪枝后正确率提高，则剪枝
        if aftacc > preacc:
            return majorityCnt(D, l)
    
    # 如果剪枝后正确率未提高，则不剪枝
    for value in values:    # 标称型的属性值有几种，就要几个子树。
        B = A[:]
        subD = D[D[bestFeature] == value]
        subval = val[val[bestFeature] == value]
        myTree[txt][value] =AfterTreeGenerate(subD, subval, B, model)
    return myTree

"""# 预测结果"""

def decideTreePredict(decideTree, testData, labelsFull):
    firstFeat = list(decideTree.keys())[0]
    secDict = decideTree[firstFeat]
    featIndex = labelsFull.index(firstFeat)
    classLabel = None
    for value in secDict.keys():
        if testData[featIndex] == value:
            if type(secDict[value]).__name__ == 'dict':
                classLabel = decideTreePredict(secDict[value], testData, labelsFull)
            else:
                classLabel = secDict[value]
    return classLabel

"""# 计算正确率"""

def get_acc(tree, val, l="target"):
    B = [i+"=?" for i in val.columns[:-1]]
    acc = 0
    for index, row in val.iterrows():
        pred = decideTreePredict(tree, row, B)
        if pred == row[l]:
            acc = acc + 1
    return float(acc) / len(val)

"""# 画图"""

root='0'
def sub_plot(g, tree, inc):
    global root
    first_label = list(tree.keys())[0]
    ts = tree[first_label]
    for i in ts.keys():
        if isinstance(tree[first_label][i], dict):
            root = str(int(root) + 1)
            g.node(root, list(tree[first_label][i].keys())[0])
            g.edge(inc, root, str(i))
            sub_plot(g, tree[first_label][i], root)
        else:
            root = str(int(root) + 1)
            g.node(root, tree[first_label][i])
            g.edge(inc, root, str(i))

def plot_model(tree, name):
    global root
    root = '0'
    g = Digraph(filename=name, format='png')
    first_label = list(tree.keys())[0]
    g.node("0", first_label)
    sub_plot(g, tree, "0")
    g.view()

"""# 主函数"""

# list(train.columns[:-1]) 为属性列表
notree_Gini_1 = TreeGenerate(train1, list(train1.columns[:-1]), 'Gini')
pretree_Gini_1= PreTreeGenerate(train1, val1, list(train1.columns[:-1]), 'Gini')
afttree_Gini_1 = AfterTreeGenerate(train1, val1, list(train1.columns[:-1]), 'Gini')

print("Gini 上数据集(1)未剪枝正确率为：%.4f" % get_acc(notree_Gini_1, val1))
print("Gini 上数据集(1)预剪枝正确率为：%.4f" % get_acc(pretree_Gini_1, val1))
print("Gini 上数据集(1)后剪枝正确率为：%.4f" % get_acc(afttree_Gini_1, val1))

notree_Gain_1 = TreeGenerate(train1, list(train1.columns[:-1]), 'Gain')
pretree_Gain_1= PreTreeGenerate(train1, val1, list(train1.columns[:-1]), 'Gain')
afttree_Gain_1 = AfterTreeGenerate(train1, val1, list(train1.columns[:-1]), 'Gain')

print("Gain 上数据集(1)未剪枝正确率为：%.4f" % get_acc(notree_Gain_1, val1))
print("Gain 上数据集(1)预剪枝正确率为：%.4f" % get_acc(pretree_Gain_1, val1))
print("Gain 上数据集(1)后剪枝正确率为：%.4f" % get_acc(afttree_Gain_1, val1))

# list(train.columns[:-1]) 为属性列表
notree_Gini_2 = TreeGenerate(train2, list(train2.columns[:-1]), 'Gini')
pretree_Gini_2= PreTreeGenerate(train2, val2, list(train2.columns[:-1]), 'Gini')
afttree_Gini_2 = AfterTreeGenerate(train2, val2, list(train2.columns[:-1]), 'Gini')

print("Gini 上数据集(2)未剪枝正确率为：%.4f" % get_acc(notree_Gini_2, val2))
print("Gini 上数据集(2)预剪枝正确率为：%.4f" % get_acc(pretree_Gini_2, val2))
print("Gini 上数据集(2)后剪枝正确率为：%.4f" % get_acc(afttree_Gini_2, val2))

notree_Gain_2 = TreeGenerate(train2, list(train2.columns[:-1]), 'Gain')
pretree_Gain_2= PreTreeGenerate(train2, val2, list(train2.columns[:-1]), 'Gain')
afttree_Gain_2 = AfterTreeGenerate(train2, val2, list(train2.columns[:-1]), 'Gain')

print("Gain 上数据集(2)未剪枝正确率为：%.4f" % get_acc(notree_Gain_2, val2))
print("Gain 上数据集(2)预剪枝正确率为：%.4f" % get_acc(pretree_Gain_2, val2))
print("Gain 上数据集(2)后剪枝正确率为：%.4f" % get_acc(afttree_Gain_2, val2))

# list(train.columns[:-1]) 为属性列表
notree_Gini_3 = TreeGenerate(train3, list(train3.columns[:-1]), 'Gini')
pretree_Gini_3= PreTreeGenerate(train3, val3, list(train3.columns[:-1]), 'Gini')
afttree_Gini_3 = AfterTreeGenerate(train3, val3, list(train3.columns[:-1]), 'Gini')

print("Gini 上数据集(3)未剪枝正确率为：%.4f" % get_acc(notree_Gini_3, val3))
print("Gini 上数据集(3)预剪枝正确率为：%.4f" % get_acc(pretree_Gini_3, val3))
print("Gini 上数据集(3)后剪枝正确率为：%.4f" % get_acc(afttree_Gini_3, val3))

notree_Gain_3 = TreeGenerate(train3, list(train3.columns[:-1]), 'Gain')
pretree_Gain_3= PreTreeGenerate(train3, val3, list(train3.columns[:-1]), 'Gain')
afttree_Gain_3 = AfterTreeGenerate(train3, val3, list(train3.columns[:-1]), 'Gain')

print("Gain 上数据集(3)未剪枝正确率为：%.4f" % get_acc(notree_Gain_3, val3))
print("Gain 上数据集(3)预剪枝正确率为：%.4f" % get_acc(pretree_Gain_3, val3))
print("Gain 上数据集(3)后剪枝正确率为：%.4f" % get_acc(afttree_Gain_3, val3))

# list(train.columns[:-1]) 为属性列表
notree_Gini_4 = TreeGenerate(train4, list(train4.columns[:-1]), 'Gini')
pretree_Gini_4= PreTreeGenerate(train4, val4, list(train4.columns[:-1]), 'Gini')
afttree_Gini_4 = AfterTreeGenerate(train4, val4, list(train4.columns[:-1]), 'Gini')

print("Gini 上数据集(4)未剪枝正确率为：%.4f" % get_acc(notree_Gini_4, val4))
print("Gini 上数据集(4)预剪枝正确率为：%.4f" % get_acc(pretree_Gini_4, val4))
print("Gini 上数据集(4)后剪枝正确率为：%.4f" % get_acc(afttree_Gini_4, val4))

notree_Gain_4 = TreeGenerate(train4, list(train4.columns[:-1]), 'Gain')
pretree_Gain_4= PreTreeGenerate(train4, val4, list(train4.columns[:-1]), 'Gain')
afttree_Gain_4 = AfterTreeGenerate(train4, val4, list(train4.columns[:-1]), 'Gain')

print("Gain 上数据集(4)未剪枝正确率为：%.4f" % get_acc(notree_Gain_4, val4))
print("Gain 上数据集(4)预剪枝正确率为：%.4f" % get_acc(pretree_Gain_4, val4))
print("Gain 上数据集(4)后剪枝正确率为：%.4f" % get_acc(afttree_Gain_4, val4))
# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14c8bMu0eAnMBmc7rQGeFi_pokotbTrka

# 导包
"""

import copy
import numpy as np
import pandas as pd
from graphviz import Digraph

"""# 数据预处理"""

val = pd.read_csv('watermelon_val.csv')
train = pd.read_csv('watermelon_train.csv')

train.head()

"""# 定义决策树

## 计算基尼值
"""

def get_Gini(D, l):
    # D为数据集，l为标签所在列的名称
    labels = list(set(D[l]))
    Gini = 1
    for label in labels:
        p = len(D[D[l] == label]) / float(len(D))
        Gini = Gini - p * p
    return Gini

"""## 计算基尼指数"""

def get_Gini_index(D, a, l):
    # D为数据集，a为属性值，l为标签所在列的名称
    labels = list(set(D[a]))
    Gini = 0
    for label in labels:
        D_ = D[D[a] == label]
        tmp = len(D_) / float(len(D)) * get_Gini(D_, l)
        Gini = Gini + tmp
    return Gini

"""## 寻找基尼指数最小的划分数据集的方式"""

def chooseBestSplit(D, A, l):
    Ginis = []
    for a in A:
        Ginis.append(get_Gini_index(D, a, l))
    # bestFeature: 使得到最大增益划分的属性。
    bestFeature = A[(np.argmin(Ginis))]
    return bestFeature

"""## 投票"""

def majorityCnt(D, l):
    if len(D) == 0:
        return "好瓜"     
    else:
        labels = list(set(D[l]))
        counts = []
        for label in labels:
            count = len(D[D[l] == label])
            counts.append(count)
        return labels[np.argmax(counts)]

"""## 生成未剪枝决策树"""

def TreeGenerate(D, A, l="好瓜"):
    # 如果D为空，则返回默认值
    if len(D) == 0:
        return '好瓜'
    
    # 如果剩余的类别全相同,则返回
    if len(list(set(D[l]))) == 1:
        return list(set(D[l]))[0]
    
    # 如果只剩下类标签，投票返回
    if len(A) == 0:
        return majorityCnt(D, l)

    # 得到增益最大划分的属性
    bestFeature = chooseBestSplit(D, A, l)
    
    # 创建属性标注
    txt = bestFeature + "=" + "?"

    # 创建节点
    myTree = {txt: {}}

    A.remove(bestFeature) # 生成子树的时候要将已遍历的属性删去。
    
    global train
    values = list(set(train[bestFeature]))  # 最好的特征的类别列表
    for value in values:    # 标称型的属性值有几种，就要几个子树。
        B = A[:]
        subD = D[D[bestFeature] == value]
        myTree[txt][value] = TreeGenerate(subD, B)
    return myTree

"""## 生成预剪枝决策树"""

def PreTreeGenerate(D, val, A, l="好瓜"):
    # 如果D为空，则返回默认值
    if len(D) == 0:
        return '好瓜'
    
    # 如果剩余的类别全相同,则返回
    if len(list(set(D[l]))) == 1:
        return list(set(D[l]))[0]
    
    # 如果只剩下类标签，投票返回
    if len(A) == 0:
        return majorityCnt(D, l)
    
    # 得到增益最大划分的属性、阈值
    bestFeature = chooseBestSplit(D, A, l)
    
    # 创建属性标注
    txt = bestFeature + "=" + "?"

    # 创建节点
    myTree = {txt: {}}

    A.remove(bestFeature) # 生成子树的时候要将已遍历的属性删去。数值型不要删除。
    
    # 计算剪枝后正确率
    label = majorityCnt(D, l)
    aacc = 0
    for index, row in val.iterrows():
        if label == row[l]:
            aacc = aacc + 1
    if len(val):
        aftacc = float(aacc) / len(val)
    else:
        aftacc = 0

    global train
    values = list(set(train[bestFeature]))  # 最好的特征的类别列表
    
    # 计算剪枝前正确率
    pacc = 0
    for value in values:
        subD = D[D[bestFeature] == value]
        subval = val[val[bestFeature] == value]
        label = majorityCnt(subD, l)
        for index, row in subval.iterrows():
            if label == row[l]:
                pacc = pacc + 1
    if len(val):
        preacc = float(pacc) / len(val)
    else:
        preacc = 0
    
    # 如果剪枝后正确率未提高，则不剪枝，否则剪枝
    if aftacc < preacc:
        for value in values:    # 标称型的属性值有几种，就要几个子树。
            B = A[:]
            subD = D[D[bestFeature] == value]
            subval = val[val[bestFeature] == value]
            myTree[txt][value] = PreTreeGenerate(subD, subval, B)
        return myTree
    else:
        return majorityCnt(D, l)

"""## 生成后剪枝决策树

"""

def AfterTreeGenerate(D, val, A, l="好瓜"):
    # 如果D为空，则返回默认值
    if len(D) == 0:
        return '好瓜'
    
    # 如果剩余的类别全相同,则返回
    if len(list(set(D[l]))) == 1:
        return list(set(D[l]))[0]
    
    # 如果只剩下类标签，投票返回
    if len(A) == 0:
        return majorityCnt(D, l)

    # 得到增益最大划分的属性、阈值
    bestFeature = chooseBestSplit(D, A, l)
    
    # 创建属性标注
    txt = bestFeature + "=" + "?"

    # 创建节点
    myTree = {txt: {}}
    A.remove(bestFeature) # 生成子树的时候要将已遍历的属性删去。数值型不要删除。
    
    global train
    values = list(set(train[bestFeature]))  # 最好的特征的类别列表
    
    # 判断子节点是否为叶节点
    F = 1
    for value in values:
        B = A[:]
        subD = D[D[bestFeature] == value]
        subval = val[val[bestFeature] == value]
        if isinstance(AfterTreeGenerate(subD, subval, B), dict):
            F = 0

    if F==1:
        # 计算剪枝后正确率
        label = majorityCnt(D, l)
        aacc = 0
        for index, row in val.iterrows():
            if label == row[l]:
                aacc = aacc + 1
        if len(val):
            aftacc = float(aacc) / len(val)
        else:
            aftacc = 0
    
        # 计算剪枝前正确率
        pacc = 0
        for value in values:
            subD = D[D[bestFeature] == value]
            subval = val[val[bestFeature] == value]
            label = majorityCnt(subD, l)
            for index, row in subval.iterrows():
                if label == row[l]:
                    pacc = pacc + 1
        if len(val):
            preacc = float(pacc) / len(val)
        else:
            preacc = 0
        
        # 如果剪枝后正确率提高，则剪枝
        if aftacc > preacc:
            return majorityCnt(D, l)
    
    # 如果剪枝后正确率未提高，则不剪枝
    for value in values:    # 标称型的属性值有几种，就要几个子树。
        B = A[:]
        subD = D[D[bestFeature] == value]
        subval = val[val[bestFeature] == value]
        myTree[txt][value] =AfterTreeGenerate(subD, subval, B)
    return myTree

"""# 画图"""

def sub_plot(g, tree, inc):
    global root
    first_label = list(tree.keys())[0]
    ts = tree[first_label]
    for i in ts.keys():
        if isinstance(tree[first_label][i], dict):
            root = str(int(root) + 1)
            g.node(root, list(tree[first_label][i].keys())[0], fontname="FangSong")
            g.edge(inc, root, str(i), fontname="FangSong")
            sub_plot(g, tree[first_label][i], root)
        else:
            root = str(int(root) + 1)
            g.node(root, tree[first_label][i], fontname="FangSong")
            g.edge(inc, root, str(i), fontname="FangSong")

def plot_model(tree, name):
    global root
    root = '0'
    g = Digraph(filename=name, format='png')
    first_label = list(tree.keys())[0]
    g.node("0", first_label, fontname="FangSong")
    sub_plot(g, tree, "0")
    g.view()

"""# 预测结果"""

def decideTreePredict(decideTree, testData, labelsFull):
    firstFeat = list(decideTree.keys())[0]
    secDict = decideTree[firstFeat]
    featIndex = labelsFull.index(firstFeat)
    classLabel = None
    for value in secDict.keys():
        if testData[featIndex] == value:
            if type(secDict[value]).__name__ == 'dict':
                classLabel = decideTreePredict(secDict[value], testData, labelsFull)
            else:
                classLabel = secDict[value]
    return classLabel

"""# 计算正确率"""

def get_acc(tree, val, l="好瓜"):
    B = [i+"=?" for i in val.columns[:-1]]
    acc = 0
    for index, row in val.iterrows():
        pred = decideTreePredict(tree, row, B)
        if pred == row[l]:
            acc = acc + 1
    return float(acc) / len(val)

"""# 主函数"""

A = ['脐部', '色泽', '根蒂', '敲声', '纹理', '触感']
notree = TreeGenerate(train, A[:])
pretree= PreTreeGenerate(train, val, A[:])
afttree = AfterTreeGenerate(train, val, A[:])

print("未剪枝正确率为：%.4f" % get_acc(notree, val))
print("预剪枝正确率为：%.4f" % get_acc(pretree, val))
print("后剪枝正确率为：%.4f" % get_acc(afttree, val))

plot_model(notree, "未剪枝决策树")
plot_model(pretree, "预剪枝决策树")
plot_model(afttree, "后剪枝决策树")
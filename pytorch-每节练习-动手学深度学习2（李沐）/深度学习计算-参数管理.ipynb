{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化。\n",
    "* 参数初始化。\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1385],\n",
       "        [0.1398]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-2.6647e-02,  3.2108e-02, -2.7277e-01,  1.7047e-04,  2.5038e-01,\n",
      "         -9.3879e-02, -2.5590e-01, -1.4587e-01]])), ('bias', tensor([0.3195]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.3195], requires_grad=True)\n",
      "tensor([0.3195])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。\n",
    "这就是我们需要显式参数值的原因。\n",
    "除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])#*拆解变量\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 19
   },
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3248])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2676],\n",
       "        [-0.2679]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套4个block1()\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "[**设计了网络后，我们看看它是如何工作的。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3981, -0.4271, -0.3701,  0.1086, -0.1413,  0.3649,  0.0478, -0.4952])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 35
   },
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "PyTorch的`nn.init`模块提供了多种预置初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 39
   },
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0053,  0.0067,  0.0123,  0.0004]), tensor(0.))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 43
   },
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4398, -0.2092,  0.2993,  0.2878])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "同样，我们实现了一个`my_init`函数来应用到`net`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 56,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -8.5068,  0.0000,  6.6374],\n",
       "        [ 0.0000,  5.2751, -7.4942,  5.9510]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 58
   },
   "source": [
    "注意，我们始终可以直接设置参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "origin_pos": 60,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000, -7.5068,  1.0000,  7.6374])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 63
   },
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "origin_pos": 65,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 68,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "这个例子表明第三个和第五个神经网络层的参数是绑定的。\n",
    "它们不仅值相等，而且由相同的张量表示。\n",
    "因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "你可能会思考：当参数绑定时，梯度会发生什么情况？\n",
    "答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层\n",
    "（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 69
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2564, grad_fn=<SumBackward0>),\n",
       " OrderedDict([('linear.weight',\n",
       "               tensor([[ 1.5743e-01, -1.2441e-01, -1.0412e-01, -1.6669e-01,  4.1038e-02,\n",
       "                         9.0181e-02, -1.9396e-02,  1.9009e-01, -1.5538e-01,  3.5291e-03,\n",
       "                        -7.1080e-02,  6.0398e-03,  1.8161e-02, -5.0643e-03,  2.5993e-02,\n",
       "                         1.3811e-01,  9.6099e-02, -8.6608e-02,  1.4026e-01,  1.5926e-01],\n",
       "                       [-6.0259e-02,  3.0180e-02, -1.9588e-01,  6.1577e-03,  1.9956e-01,\n",
       "                         1.9762e-01, -1.9378e-01, -2.1243e-01, -7.0887e-02,  1.3863e-01,\n",
       "                        -2.0732e-01, -7.5793e-02,  1.9349e-01,  1.6408e-02, -1.1358e-01,\n",
       "                         4.1340e-02,  9.0134e-02, -5.9275e-02,  7.7706e-02, -3.7947e-02],\n",
       "                       [ 1.5348e-02, -1.6496e-01,  1.7909e-01, -1.5274e-01, -5.6120e-02,\n",
       "                         9.9634e-02, -2.5748e-02, -1.8370e-01,  4.7934e-02, -2.2346e-02,\n",
       "                        -1.2349e-01, -3.7523e-02,  9.7004e-02,  1.1741e-01, -1.6990e-02,\n",
       "                        -3.0890e-02,  2.9248e-02, -2.1559e-01,  2.2220e-02,  1.4727e-01],\n",
       "                       [-1.5912e-01, -6.4542e-02,  1.1777e-01, -1.7463e-01, -1.5194e-01,\n",
       "                         1.1622e-01, -1.0997e-01, -8.0196e-02,  1.4196e-01,  1.0877e-01,\n",
       "                         1.6726e-01,  2.0277e-01, -5.6491e-03,  2.2258e-01,  1.7056e-02,\n",
       "                        -1.9256e-01, -9.9140e-02, -1.6817e-01, -1.7994e-01, -1.3963e-01],\n",
       "                       [-6.5889e-02,  1.7647e-01, -6.6555e-03, -1.4245e-01,  1.2368e-01,\n",
       "                         2.0606e-01,  1.6423e-04,  8.5714e-02,  1.2042e-01,  1.1855e-01,\n",
       "                         2.1868e-01, -2.3642e-02, -1.9278e-02, -3.7316e-03,  2.1524e-01,\n",
       "                         2.9353e-02, -9.9203e-02,  5.8911e-02, -2.8114e-03,  1.6745e-01],\n",
       "                       [ 1.9225e-01,  1.7888e-01,  1.9665e-01, -1.9420e-01, -6.5506e-02,\n",
       "                         1.0329e-01, -8.8346e-02,  2.1453e-01,  1.8009e-01,  9.5814e-02,\n",
       "                         1.0401e-02,  4.3447e-03, -1.8409e-02, -1.7471e-02,  2.0387e-01,\n",
       "                        -2.1538e-01,  1.8186e-01, -5.2344e-03, -3.1737e-02,  1.2122e-01],\n",
       "                       [-1.1862e-01, -6.8733e-02,  6.2791e-02, -1.9373e-02, -1.1253e-01,\n",
       "                         2.1175e-01,  1.4950e-02,  1.4427e-01,  5.2691e-02, -2.7524e-02,\n",
       "                        -7.8187e-02, -7.9591e-02, -1.8870e-01,  1.9467e-01, -8.6157e-02,\n",
       "                         2.0003e-01,  1.3812e-01,  2.2823e-02, -6.9024e-02,  8.5651e-02],\n",
       "                       [ 1.4560e-01, -1.5823e-01,  5.2435e-02,  1.9811e-01,  9.4721e-02,\n",
       "                        -2.6431e-02, -5.4860e-02,  1.6477e-01, -1.3182e-01, -2.2194e-01,\n",
       "                         8.5832e-02, -1.5023e-01, -1.9929e-01,  2.0703e-01,  5.7787e-02,\n",
       "                         1.0888e-01, -6.5935e-02,  1.6636e-02, -1.0736e-01, -1.8922e-01],\n",
       "                       [-1.1155e-01,  8.2821e-02, -4.2398e-02, -6.3869e-02,  1.0279e-01,\n",
       "                        -5.9839e-02,  1.6192e-01,  1.4118e-01,  1.4446e-01, -7.4122e-02,\n",
       "                         1.7495e-01,  2.0180e-01,  5.1645e-02, -5.3487e-02,  8.3496e-02,\n",
       "                         1.7803e-01, -2.9362e-02,  9.4734e-02,  1.4904e-01, -2.9727e-02],\n",
       "                       [-1.7430e-01, -8.6557e-02,  1.7719e-01,  1.1519e-01, -1.9281e-01,\n",
       "                        -3.0315e-02,  4.5753e-02, -9.9619e-02, -2.7769e-02,  1.5321e-01,\n",
       "                        -2.2186e-01,  9.1297e-02,  1.1366e-01,  8.0118e-02,  2.5857e-02,\n",
       "                        -4.0986e-02,  2.0463e-01,  2.0231e-01,  1.6960e-01,  1.9338e-01],\n",
       "                       [ 2.9414e-02,  5.0929e-02, -2.0971e-01,  1.3529e-01, -2.9644e-02,\n",
       "                         1.8051e-01,  2.8411e-02,  1.2204e-01,  7.8072e-02, -4.4158e-02,\n",
       "                        -1.7555e-01, -3.1852e-02, -2.0871e-02,  1.2961e-01,  4.0584e-02,\n",
       "                         8.6073e-02, -5.8415e-02,  1.0009e-01,  1.1113e-01, -1.5861e-01],\n",
       "                       [ 2.1798e-01, -6.2953e-02, -8.2598e-03,  4.6606e-02, -9.2179e-02,\n",
       "                         1.9703e-01,  6.2318e-02,  1.2392e-01, -2.1544e-01, -1.3004e-01,\n",
       "                         1.3634e-01, -5.2388e-02,  1.1797e-01,  1.9533e-01,  1.2318e-01,\n",
       "                        -8.7828e-02,  1.5762e-01, -7.6440e-02,  1.8880e-01,  1.4379e-01],\n",
       "                       [ 2.9893e-02,  1.7560e-01,  2.8888e-02, -9.6830e-02,  9.5602e-02,\n",
       "                        -1.5485e-01, -3.9357e-03,  1.8649e-02,  1.1690e-01, -1.2260e-01,\n",
       "                        -1.0273e-02,  1.6623e-01, -1.7522e-01,  6.0613e-03, -1.6193e-01,\n",
       "                        -1.0778e-01,  1.5214e-01,  1.9101e-01,  5.7342e-03,  8.4632e-02],\n",
       "                       [-1.5206e-01, -1.7974e-01, -5.7282e-02, -1.2300e-01,  1.1274e-01,\n",
       "                         2.1064e-01,  1.3735e-01,  1.2157e-01, -1.0742e-01, -8.8295e-02,\n",
       "                        -2.8444e-02, -4.0453e-02,  1.6313e-01,  1.0473e-01,  1.7136e-01,\n",
       "                         1.4836e-01, -9.3704e-02,  1.6878e-01, -5.6752e-02,  2.1773e-01],\n",
       "                       [ 1.0953e-02, -2.9774e-02, -1.9940e-01,  1.8632e-01,  1.9968e-01,\n",
       "                         1.7116e-01,  1.7970e-01,  7.0004e-02, -2.2116e-02,  1.3955e-01,\n",
       "                        -8.9058e-02,  1.2996e-01, -1.7283e-01,  4.7150e-02,  2.0394e-01,\n",
       "                        -4.1116e-02, -1.1281e-01, -1.1196e-01, -1.3490e-02,  1.3048e-01],\n",
       "                       [ 1.6871e-01, -4.7147e-02,  1.0300e-01,  6.9079e-02, -1.2228e-01,\n",
       "                         1.4945e-01,  6.8352e-02, -1.0288e-02,  2.5808e-02,  2.9334e-02,\n",
       "                        -2.7621e-02, -1.5230e-01, -4.2966e-02,  1.3156e-01,  2.2169e-01,\n",
       "                         2.1079e-01,  9.5654e-02, -1.8678e-01,  1.8413e-01, -6.4000e-02],\n",
       "                       [-1.2553e-01, -7.8336e-03, -2.1505e-01, -3.8638e-02, -1.2128e-02,\n",
       "                        -1.4865e-01,  5.4518e-02,  7.9268e-02, -1.0894e-01,  2.0406e-01,\n",
       "                        -1.7317e-01,  6.1553e-02,  1.1561e-01,  2.1243e-01,  1.2790e-01,\n",
       "                         9.7871e-02, -4.7225e-03, -7.3811e-02,  6.5832e-02,  1.3879e-01],\n",
       "                       [ 1.6569e-01, -2.1119e-01, -9.3831e-02,  2.9530e-02,  6.5399e-02,\n",
       "                        -6.4097e-02, -2.1659e-01,  1.6232e-01, -3.4881e-02,  2.6601e-02,\n",
       "                        -1.7127e-01, -1.7515e-01, -4.7852e-02,  3.0724e-02, -1.0634e-01,\n",
       "                        -2.1796e-01,  1.5266e-01,  6.8266e-02, -6.5250e-02,  2.5791e-02],\n",
       "                       [ 1.4206e-01, -1.0606e-01,  1.5294e-01, -7.3497e-02, -1.7730e-01,\n",
       "                        -1.7240e-01, -1.6253e-02, -1.4328e-01,  7.4223e-02, -1.4924e-01,\n",
       "                         1.1390e-02, -1.7514e-01, -1.8701e-01,  8.3167e-03, -1.6739e-01,\n",
       "                         1.2043e-02, -9.4418e-02, -1.9483e-01,  6.3362e-03,  3.9925e-02],\n",
       "                       [-1.0872e-01,  1.2918e-01, -2.1849e-01,  5.2180e-02, -1.1238e-01,\n",
       "                        -4.8825e-02,  4.3376e-02, -3.7082e-02,  6.6328e-02, -8.2521e-02,\n",
       "                        -8.9708e-02, -3.1475e-02, -1.3539e-01, -1.2185e-01,  2.1443e-01,\n",
       "                        -1.4358e-01, -2.7558e-02,  1.3452e-01, -2.1521e-01,  1.3722e-01]])),\n",
       "              ('linear.bias',\n",
       "               tensor([-0.0900,  0.1054,  0.1886, -0.0779,  0.1634,  0.0004,  0.1885, -0.1925,\n",
       "                        0.1040, -0.0243,  0.1003,  0.1434, -0.0257,  0.1265, -0.0065,  0.1682,\n",
       "                       -0.0631, -0.0321, -0.0963, -0.0131]))]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        # self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(X)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "            \n",
    "FancyMLP = FixedHiddenMLP()\n",
    "FancyMLP(X),FancyMLP.state_dict()#FancyMLP.linear.weight/bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module torch.nn.init in torch.nn:\n",
      "\n",
      "NAME\n",
      "    torch.nn.init\n",
      "\n",
      "FUNCTIONS\n",
      "    calculate_gain(nonlinearity, param=None)\n",
      "        Return the recommended gain value for the given nonlinearity function.\n",
      "        The values are as follows:\n",
      "        \n",
      "        ================= ====================================================\n",
      "        nonlinearity      gain\n",
      "        ================= ====================================================\n",
      "        Linear / Identity :math:`1`\n",
      "        Conv{1,2,3}D      :math:`1`\n",
      "        Sigmoid           :math:`1`\n",
      "        Tanh              :math:`\\frac{5}{3}`\n",
      "        ReLU              :math:`\\sqrt{2}`\n",
      "        Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
      "        SELU              :math:`\\frac{3}{4}`\n",
      "        ================= ====================================================\n",
      "        \n",
      "        .. warning::\n",
      "            In order to implement `Self-Normalizing Neural Networks`_ ,\n",
      "            you should use ``nonlinearity='linear'`` instead of ``nonlinearity='selu'``.\n",
      "            This gives the initial weights a variance of ``1 / N``,\n",
      "            which is necessary to induce a stable fixed point in the forward pass.\n",
      "            In contrast, the default gain for ``SELU`` sacrifices the normalisation\n",
      "            effect for more stable gradient flow in rectangular layers.\n",
      "        \n",
      "        Args:\n",
      "            nonlinearity: the non-linear function (`nn.functional` name)\n",
      "            param: optional parameter for the non-linear function\n",
      "        \n",
      "        Examples:\n",
      "            >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n",
      "        \n",
      "        .. _Self-Normalizing Neural Networks: https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html\n",
      "    \n",
      "    constant(*args, **kwargs)\n",
      "        constant(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.constant_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.constant_` for details.\n",
      "    \n",
      "    constant_(tensor: torch.Tensor, val: float) -> torch.Tensor\n",
      "        Fills the input Tensor with the value :math:`\\text{val}`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            val: the value to fill the tensor with\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.constant_(w, 0.3)\n",
      "    \n",
      "    dirac(*args, **kwargs)\n",
      "        dirac(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.dirac_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.dirac_` for details.\n",
      "    \n",
      "    dirac_(tensor, groups=1)\n",
      "        Fills the {3, 4, 5}-dimensional input `Tensor` with the Dirac\n",
      "        delta function. Preserves the identity of the inputs in `Convolutional`\n",
      "        layers, where as many input channels are preserved as possible. In case\n",
      "        of groups>1, each group of channels preserves identity\n",
      "        \n",
      "        Args:\n",
      "            tensor: a {3, 4, 5}-dimensional `torch.Tensor`\n",
      "            groups (optional): number of groups in the conv layer (default: 1)\n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 16, 5, 5)\n",
      "            >>> nn.init.dirac_(w)\n",
      "            >>> w = torch.empty(3, 24, 5, 5)\n",
      "            >>> nn.init.dirac_(w, 3)\n",
      "    \n",
      "    eye(*args, **kwargs)\n",
      "        eye(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.eye_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.eye_` for details.\n",
      "    \n",
      "    eye_(tensor)\n",
      "        Fills the 2-dimensional input `Tensor` with the identity\n",
      "        matrix. Preserves the identity of the inputs in `Linear` layers, where as\n",
      "        many inputs are preserved as possible.\n",
      "        \n",
      "        Args:\n",
      "            tensor: a 2-dimensional `torch.Tensor`\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.eye_(w)\n",
      "    \n",
      "    kaiming_normal(*args, **kwargs)\n",
      "        kaiming_normal(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.kaiming_normal_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.kaiming_normal_` for details.\n",
      "    \n",
      "    kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
      "        Fills the input `Tensor` with values according to the method\n",
      "        described in `Delving deep into rectifiers: Surpassing human-level\n",
      "        performance on ImageNet classification` - He, K. et al. (2015), using a\n",
      "        normal distribution. The resulting tensor will have values sampled from\n",
      "        :math:`\\mathcal{N}(0, \\text{std}^2)` where\n",
      "        \n",
      "        .. math::\n",
      "            \\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}\n",
      "        \n",
      "        Also known as He initialization.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            a: the negative slope of the rectifier used after this layer (only\n",
      "                used with ``'leaky_relu'``)\n",
      "            mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n",
      "                preserves the magnitude of the variance of the weights in the\n",
      "                forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n",
      "                backwards pass.\n",
      "            nonlinearity: the non-linear function (`nn.functional` name),\n",
      "                recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n",
      "    \n",
      "    kaiming_uniform(*args, **kwargs)\n",
      "        kaiming_uniform(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.kaiming_uniform_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.kaiming_uniform_` for details.\n",
      "    \n",
      "    kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')\n",
      "        Fills the input `Tensor` with values according to the method\n",
      "        described in `Delving deep into rectifiers: Surpassing human-level\n",
      "        performance on ImageNet classification` - He, K. et al. (2015), using a\n",
      "        uniform distribution. The resulting tensor will have values sampled from\n",
      "        :math:`\\mathcal{U}(-\\text{bound}, \\text{bound})` where\n",
      "        \n",
      "        .. math::\n",
      "            \\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}\n",
      "        \n",
      "        Also known as He initialization.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            a: the negative slope of the rectifier used after this layer (only\n",
      "                used with ``'leaky_relu'``)\n",
      "            mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``\n",
      "                preserves the magnitude of the variance of the weights in the\n",
      "                forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the\n",
      "                backwards pass.\n",
      "            nonlinearity: the non-linear function (`nn.functional` name),\n",
      "                recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n",
      "    \n",
      "    normal(*args, **kwargs)\n",
      "        normal(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.normal_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.normal_` for details.\n",
      "    \n",
      "    normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0) -> torch.Tensor\n",
      "        Fills the input Tensor with values drawn from the normal\n",
      "        distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            mean: the mean of the normal distribution\n",
      "            std: the standard deviation of the normal distribution\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.normal_(w)\n",
      "    \n",
      "    ones_(tensor: torch.Tensor) -> torch.Tensor\n",
      "        Fills the input Tensor with the scalar value `1`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.ones_(w)\n",
      "    \n",
      "    orthogonal(*args, **kwargs)\n",
      "        orthogonal(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.orthogonal_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.orthogonal_` for details.\n",
      "    \n",
      "    orthogonal_(tensor, gain=1)\n",
      "        Fills the input `Tensor` with a (semi) orthogonal matrix, as\n",
      "        described in `Exact solutions to the nonlinear dynamics of learning in deep\n",
      "        linear neural networks` - Saxe, A. et al. (2013). The input tensor must have\n",
      "        at least 2 dimensions, and for tensors with more than 2 dimensions the\n",
      "        trailing dimensions are flattened.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`, where :math:`n \\geq 2`\n",
      "            gain: optional scaling factor\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.orthogonal_(w)\n",
      "    \n",
      "    sparse(*args, **kwargs)\n",
      "        sparse(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.sparse_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.sparse_` for details.\n",
      "    \n",
      "    sparse_(tensor, sparsity, std=0.01)\n",
      "        Fills the 2D input `Tensor` as a sparse matrix, where the\n",
      "        non-zero elements will be drawn from the normal distribution\n",
      "        :math:`\\mathcal{N}(0, 0.01)`, as described in `Deep learning via\n",
      "        Hessian-free optimization` - Martens, J. (2010).\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            sparsity: The fraction of elements in each column to be set to zero\n",
      "            std: the standard deviation of the normal distribution used to generate\n",
      "                the non-zero values\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.sparse_(w, sparsity=0.1)\n",
      "    \n",
      "    trunc_normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0) -> torch.Tensor\n",
      "        Fills the input Tensor with values drawn from a truncated\n",
      "        normal distribution. The values are effectively drawn from the\n",
      "        normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
      "        with values outside :math:`[a, b]` redrawn until they are within\n",
      "        the bounds. The method used for generating the random values works\n",
      "        best when :math:`a \\leq \\text{mean} \\leq b`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            mean: the mean of the normal distribution\n",
      "            std: the standard deviation of the normal distribution\n",
      "            a: the minimum cutoff value\n",
      "            b: the maximum cutoff value\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.trunc_normal_(w)\n",
      "    \n",
      "    uniform(*args, **kwargs)\n",
      "        uniform(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.uniform_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.uniform_` for details.\n",
      "    \n",
      "    uniform_(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0) -> torch.Tensor\n",
      "        Fills the input Tensor with values drawn from the uniform\n",
      "        distribution :math:`\\mathcal{U}(a, b)`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            a: the lower bound of the uniform distribution\n",
      "            b: the upper bound of the uniform distribution\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.uniform_(w)\n",
      "    \n",
      "    xavier_normal(*args, **kwargs)\n",
      "        xavier_normal(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.xavier_normal_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.xavier_normal_` for details.\n",
      "    \n",
      "    xavier_normal_(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor\n",
      "        Fills the input `Tensor` with values according to the method\n",
      "        described in `Understanding the difficulty of training deep feedforward\n",
      "        neural networks` - Glorot, X. & Bengio, Y. (2010), using a normal\n",
      "        distribution. The resulting tensor will have values sampled from\n",
      "        :math:`\\mathcal{N}(0, \\text{std}^2)` where\n",
      "        \n",
      "        .. math::\n",
      "            \\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}\n",
      "        \n",
      "        Also known as Glorot initialization.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            gain: an optional scaling factor\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.xavier_normal_(w)\n",
      "    \n",
      "    xavier_uniform(*args, **kwargs)\n",
      "        xavier_uniform(...)\n",
      "        \n",
      "        .. warning::\n",
      "            This method is now deprecated in favor of :func:`torch.nn.init.xavier_uniform_`.\n",
      "        \n",
      "        See :func:`~torch.nn.init.xavier_uniform_` for details.\n",
      "    \n",
      "    xavier_uniform_(tensor: torch.Tensor, gain: float = 1.0) -> torch.Tensor\n",
      "        Fills the input `Tensor` with values according to the method\n",
      "        described in `Understanding the difficulty of training deep feedforward\n",
      "        neural networks` - Glorot, X. & Bengio, Y. (2010), using a uniform\n",
      "        distribution. The resulting tensor will have values sampled from\n",
      "        :math:`\\mathcal{U}(-a, a)` where\n",
      "        \n",
      "        .. math::\n",
      "            a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}\n",
      "        \n",
      "        Also known as Glorot initialization.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "            gain: an optional scaling factor\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n",
      "    \n",
      "    zeros_(tensor: torch.Tensor) -> torch.Tensor\n",
      "        Fills the input Tensor with the scalar value `0`.\n",
      "        \n",
      "        Args:\n",
      "            tensor: an n-dimensional `torch.Tensor`\n",
      "        \n",
      "        Examples:\n",
      "            >>> w = torch.empty(3, 5)\n",
      "            >>> nn.init.zeros_(w)\n",
      "\n",
      "FILE\n",
      "    c:\\programdata\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\init.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.\n",
    "help(nn.init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 共享参数通常可以节省内存，并在以下方面具有特定的好处：\n",
    "\n",
    "对于图像识别中的CNN，共享参数使网络能够在图像中的任何地方而不是仅在某个区域中查找给定的功能。\n",
    "对于RNN，它在序列的各个时间步之间共享参数，因此可以很好地推广到不同序列长度的示例。\n",
    "对于自动编码器，编码器和解码器共享参数。 在具有线性激活的单层自动编码器中，共享权重会在权重矩阵的不同隐藏层之间强制正交。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
